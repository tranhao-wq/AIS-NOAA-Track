import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import folium
from folium.plugins import HeatMap, MarkerCluster
import matplotlib.pyplot as plt
import io
import base64

def detect_vessel_patterns(df):
    """Ph√°t hi·ªán m·∫´u di chuy·ªÉn b·∫•t th∆∞·ªùng c·ªßa t√†u"""
    try:
        # T√¨m c·ªôt t·ªça ƒë·ªô
        lat_col = next((col for col in ['LAT', 'Latitude', 'lat', 'latitude'] if col in df.columns), None)
        lon_col = next((col for col in ['LON', 'Longitude', 'lon', 'longitude'] if col in df.columns), None)
        
        if not lat_col or not lon_col:
            return {"error": "Kh√¥ng t√¨m th·∫•y c·ªôt t·ªça ƒë·ªô"}
        
        # L·ªçc d·ªØ li·ªáu h·ª£p l·ªá
        df_clean = df.dropna(subset=[lat_col, lon_col])
        df_clean = df_clean[(df_clean[lat_col] >= -90) & (df_clean[lat_col] <= 90)]
        df_clean = df_clean[(df_clean[lon_col] >= -180) & (df_clean[lon_col] <= 180)]
        
        if len(df_clean) < 10:
            return {"error": "Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch"}
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        coords = df_clean[[lat_col, lon_col]].values
        coords_scaled = StandardScaler().fit_transform(coords)
        
        # Ph√°t hi·ªán c·ª•m b·∫±ng DBSCAN
        db = DBSCAN(eps=0.3, min_samples=5).fit(coords_scaled)
        labels = db.labels_
        
        # S·ªë l∆∞·ª£ng c·ª•m (kh√¥ng t√≠nh nhi·ªÖu)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        
        # Th√™m nh√£n c·ª•m v√†o d·ªØ li·ªáu
        df_clean['cluster'] = labels
        
        # T√≠nh to√°n trung t√¢m c·ªßa c√°c c·ª•m
        clusters = []
        for i in range(n_clusters):
            cluster_points = df_clean[df_clean['cluster'] == i]
            center_lat = cluster_points[lat_col].mean()
            center_lon = cluster_points[lon_col].mean()
            size = len(cluster_points)
            clusters.append({
                'id': i,
                'center': [center_lat, center_lon],
                'size': size,
                'points': len(cluster_points)
            })
        
        # T√≠nh t·ª∑ l·ªá ƒëi·ªÉm nhi·ªÖu
        noise_ratio = np.sum(labels == -1) / len(labels)
        
        return {
            "clusters": clusters,
            "n_clusters": n_clusters,
            "noise_ratio": noise_ratio,
            "total_points": len(df_clean)
        }
    except Exception as e:
        return {"error": str(e)}

def predict_vessel_density(df):
    """D·ª± ƒëo√°n m·∫≠t ƒë·ªô t√†u thuy·ªÅn trong khu v·ª±c"""
    try:
        # T√¨m c·ªôt t·ªça ƒë·ªô
        lat_col = next((col for col in ['LAT', 'Latitude', 'lat', 'latitude'] if col in df.columns), None)
        lon_col = next((col for col in ['LON', 'Longitude', 'lon', 'longitude'] if col in df.columns), None)
        
        if not lat_col or not lon_col:
            return {"error": "Kh√¥ng t√¨m th·∫•y c·ªôt t·ªça ƒë·ªô"}
        
        # L·ªçc d·ªØ li·ªáu h·ª£p l·ªá
        df_clean = df.dropna(subset=[lat_col, lon_col])
        df_clean = df_clean[(df_clean[lat_col] >= -90) & (df_clean[lat_col] <= 90)]
        df_clean = df_clean[(df_clean[lon_col] >= -180) & (df_clean[lon_col] <= 180)]
        
        if len(df_clean) < 10:
            return {"error": "Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch"}
        
        # T·∫°o l∆∞·ªõi m·∫≠t ƒë·ªô
        lat_min, lat_max = df_clean[lat_col].min(), df_clean[lat_col].max()
        lon_min, lon_max = df_clean[lon_col].min(), df_clean[lon_col].max()
        
        # T·∫°o l∆∞·ªõi 10x10
        lat_bins = np.linspace(lat_min, lat_max, 11)
        lon_bins = np.linspace(lon_min, lon_max, 11)
        
        # ƒê·∫øm s·ªë l∆∞·ª£ng t√†u trong m·ªói √¥ l∆∞·ªõi
        density_grid = np.zeros((10, 10))
        for i in range(10):
            for j in range(10):
                density_grid[i, j] = np.sum(
                    (df_clean[lat_col] >= lat_bins[i]) & 
                    (df_clean[lat_col] < lat_bins[i+1]) & 
                    (df_clean[lon_col] >= lon_bins[j]) & 
                    (df_clean[lon_col] < lon_bins[j+1])
                )
        
        # T·∫°o d·ªØ li·ªáu heatmap
        heatmap_data = []
        for i in range(10):
            for j in range(10):
                if density_grid[i, j] > 0:
                    heatmap_data.append([
                        (lat_bins[i] + lat_bins[i+1]) / 2,
                        (lon_bins[j] + lon_bins[j+1]) / 2,
                        float(density_grid[i, j])
                    ])
        
        # T√¨m c√°c khu v·ª±c c√≥ m·∫≠t ƒë·ªô cao
        high_density_areas = []
        threshold = np.percentile(density_grid[density_grid > 0], 75)  # Ng∆∞·ª°ng 75%
        for i in range(10):
            for j in range(10):
                if density_grid[i, j] > threshold:
                    high_density_areas.append({
                        'center': [
                            (lat_bins[i] + lat_bins[i+1]) / 2,
                            (lon_bins[j] + lon_bins[j+1]) / 2
                        ],
                        'density': float(density_grid[i, j]),
                        'bounds': [
                            [lat_bins[i], lon_bins[j]],
                            [lat_bins[i+1], lon_bins[j+1]]
                        ]
                    })
        
        return {
            "heatmap_data": heatmap_data,
            "high_density_areas": high_density_areas,
            "max_density": float(np.max(density_grid)),
            "avg_density": float(np.mean(density_grid[density_grid > 0]))
        }
    except Exception as e:
        return {"error": str(e)}

def analyze_vessel_types(df):
    """Ph√¢n t√≠ch chi ti·∫øt theo lo·∫°i t√†u"""
    try:
        # T√¨m c·ªôt lo·∫°i t√†u
        vessel_col = next((col for col in ['VesselType', 'ShipType', 'vessel_type'] if col in df.columns), None)
        
        if not vessel_col:
            return {"error": "Kh√¥ng t√¨m th·∫•y c·ªôt lo·∫°i t√†u"}
        
        # ƒê·∫øm s·ªë l∆∞·ª£ng theo lo·∫°i t√†u
        vessel_counts = df[vessel_col].value_counts().to_dict()
        
        # T√¨m c·ªôt t·ªëc ƒë·ªô
        speed_col = next((col for col in ['SOG', 'Speed', 'speed'] if col in df.columns), None)
        
        speed_stats = {}
        if speed_col:
            # Th·ªëng k√™ t·ªëc ƒë·ªô theo lo·∫°i t√†u
            for vessel_type in vessel_counts.keys():
                vessel_data = df[df[vessel_col] == vessel_type]
                if len(vessel_data) > 0:
                    speed_stats[vessel_type] = {
                        'avg_speed': float(vessel_data[speed_col].mean()),
                        'max_speed': float(vessel_data[speed_col].max()),
                        'min_speed': float(vessel_data[speed_col].min())
                    }
        
        # T·∫°o bi·ªÉu ƒë·ªì ph√¢n b·ªë lo·∫°i t√†u
        plt.figure(figsize=(10, 6))
        plt.bar(vessel_counts.keys(), vessel_counts.values())
        plt.title('Ph√¢n b·ªë lo·∫°i t√†u')
        plt.xlabel('Lo·∫°i t√†u')
        plt.ylabel('S·ªë l∆∞·ª£ng')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # Chuy·ªÉn bi·ªÉu ƒë·ªì th√†nh base64 ƒë·ªÉ hi·ªÉn th·ªã tr√™n web
        buf = io.BytesIO()
        plt.savefig(buf, format='png')
        buf.seek(0)
        chart = base64.b64encode(buf.read()).decode('utf-8')
        plt.close()
        
        return {
            "vessel_counts": vessel_counts,
            "speed_stats": speed_stats,
            "chart": chart
        }
    except Exception as e:
        return {"error": str(e)}

def generate_advanced_map(df):
    """T·∫°o b·∫£n ƒë·ªì n√¢ng cao v·ªõi nhi·ªÅu l·ªõp d·ªØ li·ªáu s·ª≠ d·ª•ng Leaflet"""
    try:
        # T√¨m c·ªôt t·ªça ƒë·ªô
        lat_col = next((col for col in ['LAT', 'Latitude', 'lat', 'latitude'] if col in df.columns), None)
        lon_col = next((col for col in ['LON', 'Longitude', 'lon', 'longitude'] if col in df.columns), None)
        
        if not lat_col or not lon_col:
            return "<div>Kh√¥ng t√¨m th·∫•y c·ªôt t·ªça ƒë·ªô</div>"
        
        # L·ªçc d·ªØ li·ªáu h·ª£p l·ªá
        df_clean = df.dropna(subset=[lat_col, lon_col])
        df_clean = df_clean[(df_clean[lat_col] >= -90) & (df_clean[lat_col] <= 90)]
        df_clean = df_clean[(df_clean[lon_col] >= -180) & (df_clean[lon_col] <= 180)]
        
        if len(df_clean) < 1:
            return "<div>Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t·∫°o b·∫£n ƒë·ªì</div>"
        
        # T√≠nh to√°n trung t√¢m b·∫£n ƒë·ªì
        center_lat = df_clean[lat_col].mean()
        center_lon = df_clean[lon_col].mean()
        
        # T·∫°o b·∫£n ƒë·ªì s·ª≠ d·ª•ng Leaflet
        map_html = f'''
        <div id="advanced-vessel-map" style="height: 600px; width: 100%; border-radius: 10px;"></div>
        <script>
            // Ki·ªÉm tra xem Leaflet ƒë√£ ƒë∆∞·ª£c t·∫£i ch∆∞a
            if (typeof L === 'undefined') {{            
                // T·∫£i Leaflet CSS v√† JavaScript
                var leafletCSS = document.createElement('link');
                leafletCSS.rel = 'stylesheet';
                leafletCSS.href = 'https://unpkg.com/leaflet@1.7.1/dist/leaflet.css';
                document.head.appendChild(leafletCSS);
                
                var leafletJS = document.createElement('script');
                leafletJS.src = 'https://unpkg.com/leaflet@1.7.1/dist/leaflet.js';
                document.head.appendChild(leafletJS);
                
                // T·∫£i Leaflet.heat cho b·∫£n ƒë·ªì nhi·ªát
                var leafletHeatJS = document.createElement('script');
                leafletHeatJS.src = 'https://unpkg.com/leaflet.heat@0.2.0/dist/leaflet-heat.js';
                document.head.appendChild(leafletHeatJS);
                
                // T·∫£i Leaflet.markercluster
                var markerClusterCSS = document.createElement('link');
                markerClusterCSS.rel = 'stylesheet';
                markerClusterCSS.href = 'https://unpkg.com/leaflet.markercluster@1.4.1/dist/MarkerCluster.css';
                document.head.appendChild(markerClusterCSS);
                
                var markerClusterDefaultCSS = document.createElement('link');
                markerClusterDefaultCSS.rel = 'stylesheet';
                markerClusterDefaultCSS.href = 'https://unpkg.com/leaflet.markercluster@1.4.1/dist/MarkerCluster.Default.css';
                document.head.appendChild(markerClusterDefaultCSS);
                
                var markerClusterJS = document.createElement('script');
                markerClusterJS.src = 'https://unpkg.com/leaflet.markercluster@1.4.1/dist/leaflet.markercluster.js';
                document.head.appendChild(markerClusterJS);
                
                // ƒê·ª£i Leaflet v√† c√°c plugin t·∫£i xong
                var checkInterval = setInterval(function() {{
                    if (typeof L !== 'undefined' && 
                        typeof L.markerClusterGroup !== 'undefined' && 
                        typeof L.heatLayer !== 'undefined') {{
                        clearInterval(checkInterval);
                        initAdvancedMap();
                    }}
                }}, 100);
            }} else {{
                // Leaflet ƒë√£ ƒë∆∞·ª£c t·∫£i, kh·ªüi t·∫°o b·∫£n ƒë·ªì ngay l·∫≠p t·ª©c
                initAdvancedMap();
            }}
            
            function initAdvancedMap() {{
                // T·∫°o b·∫£n ƒë·ªì
                var map = L.map('advanced-vessel-map').setView([{center_lat}, {center_lon}], 8);
                
                // Th√™m l·ªõp b·∫£n ƒë·ªì n·ªÅn
                var baseLayer = L.tileLayer('https://{{s}}.tile.openstreetmap.org/{{z}}/{{x}}/{{y}}.png', {{
                    attribution: '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
                }}).addTo(map);
                
                // M√†u s·∫Øc cho c√°c lo·∫°i t√†u
                var vesselColors = {{
                    'Cargo': '#3388ff',
                    'Tanker': '#dc3545',
                    'Passenger': '#28a745',
                    'Fishing': '#fd7e14',
                    'Tug': '#6f42c1',
                    'Military': '#000000',
                    'Sailing': '#e83e8c',
                    'Unknown': '#6c757d'
                }};
                
                // T·∫°o c√°c l·ªõp cho t·ª´ng lo·∫°i t√†u
                var vesselLayers = {{}};                
                var allMarkers = [];
                var heatData = [];
        '''
        
        # T√¨m c·ªôt lo·∫°i t√†u
        vessel_col = next((col for col in ['VesselType', 'ShipType', 'vessel_type'] if col in df.columns), None)
        
        # Gi·ªõi h·∫°n s·ªë ƒëi·ªÉm ƒë·ªÉ tr√°nh qu√° t·∫£i
        max_points = min(1000, len(df_clean))
        df_sample = df_clean.sample(n=max_points) if len(df_clean) > max_points else df_clean
        
        # T·∫°o c√°c l·ªõp cho t·ª´ng lo·∫°i t√†u
        vessel_types = []
        if vessel_col:
            vessel_types = df_clean[vessel_col].unique()
            vessel_types = [str(vt) for vt in vessel_types if pd.notna(vt)]
            
            # Th√™m m√£ JavaScript ƒë·ªÉ t·∫°o c√°c l·ªõp
            for vessel_type in vessel_types:
                map_html += f'''
                vesselLayers['{vessel_type}'] = L.layerGroup();
                '''
        
        # Th√™m c√°c ƒëi·ªÉm v√†o b·∫£n ƒë·ªì
        map_html += '''
                // T·∫°o c·ª•m marker
                var markerCluster = L.markerClusterGroup();
                
                // D·ªØ li·ªáu ƒëi·ªÉm
                var points = [
        '''
        
        # Th√™m c√°c ƒëi·ªÉm v√†o danh s√°ch
        points_data = []
        for _, row in df_sample.iterrows():
            vessel_type = str(row.get(vessel_col, 'Unknown')) if vessel_col else 'Unknown'
            vessel_type = vessel_type.replace("'", "\\'")
            
            # T·∫°o popup text
            popup_text = f"V·ªã tr√≠: {row[lat_col]:.4f}, {row[lon_col]:.4f}"
            if vessel_col:
                popup_text += f"<br>Lo·∫°i t√†u: {vessel_type}"
            
            # Th√™m th√¥ng tin kh√°c
            for col in ['MMSI', 'VesselName', 'SOG', 'COG', 'BaseDateTime']:
                if col in row and pd.notna(row[col]):
                    popup_text += f"<br>{col}: {row[col]}"
            
            # Th√™m ƒëi·ªÉm v√†o danh s√°ch
            points_data.append(f"[{row[lat_col]}, {row[lon_col]}, '{vessel_type}', '{popup_text}']")
        
        # Th√™m c√°c ƒëi·ªÉm v√†o m√£ JavaScript
        map_html += ',\n                    '.join(points_data)
        
        # Ho√†n th√†nh m√£ JavaScript
        map_html += '''
                ];
                
                // Th√™m c√°c ƒëi·ªÉm v√†o b·∫£n ƒë·ªì
                points.forEach(function(point) {
                    var lat = point[0];
                    var lon = point[1];
                    var type = point[2];
                    var popupText = point[3];
                    
                    var color = vesselColors[type] || vesselColors['Unknown'];
                    
                    var marker = L.circleMarker([lat, lon], {
                        radius: 5,
                        color: color,
                        fillColor: color,
                        fillOpacity: 0.7,
                        weight: 2
                    }).bindPopup(popupText);
                    
                    // Th√™m v√†o c·ª•m marker
                    markerCluster.addLayer(marker);
                    
                    // Th√™m v√†o l·ªõp t∆∞∆°ng ·ª©ng
                    if (vesselLayers[type]) {
                        vesselLayers[type].addLayer(marker);
                    }
                    
                    // Th√™m v√†o danh s√°ch t·∫•t c·∫£ marker
                    allMarkers.push(marker);
                    
                    // Th√™m v√†o d·ªØ li·ªáu heatmap
                    heatData.push([lat, lon, 0.5]);
                });
                
                // Th√™m c·ª•m marker v√†o b·∫£n ƒë·ªì
                map.addLayer(markerCluster);
                
                // T·∫°o heatmap
                var heatLayer = L.heatLayer(heatData, {
                    radius: 20,
                    blur: 15,
                    maxZoom: 10,
                    max: 1.0,
                    gradient: {0.4: 'blue', 0.65: 'yellow', 0.9: 'red'}
                });
                
                // T·∫°o c√°c l·ªõp c∆° s·ªü
                var baseLayers = {
                    "B·∫£n ƒë·ªì n·ªÅn": baseLayer
                };
                
                // T·∫°o c√°c l·ªõp ph·ªß
                var overlays = {
                    "T·∫•t c·∫£ t√†u": markerCluster,
                    "B·∫£n ƒë·ªì nhi·ªát": heatLayer
                };
                
                // Th√™m c√°c l·ªõp lo·∫°i t√†u v√†o overlays
                for (var type in vesselLayers) {
                    overlays["T√†u " + type] = vesselLayers[type];
                }
                
                // Th√™m ƒëi·ªÅu khi·ªÉn l·ªõp
                L.control.layers(baseLayers, overlays).addTo(map);
                
                // Th√™m ch√∫ th√≠ch
                var legend = L.control({position: 'bottomright'});
                legend.onAdd = function(map) {
                    var div = L.DomUtil.create('div', 'info legend');
                    div.style.backgroundColor = 'white';
                    div.style.padding = '10px';
                    div.style.borderRadius = '5px';
                    div.style.boxShadow = '0 0 15px rgba(0,0,0,0.2)';
                    
                    div.innerHTML = '<h4 style="margin-top: 0;">üö¢ Lo·∫°i t√†u</h4>';
                    
                    // Th√™m c√°c lo·∫°i t√†u v√†o ch√∫ th√≠ch
                    for (var type in vesselColors) {
                        if (type in vesselLayers || type === 'Unknown') {
                            div.innerHTML += 
                                '<div><span style="display:inline-block; width:15px; height:15px; border-radius:50%; background:' + 
                                vesselColors[type] + ';"></span> ' + type + '</div>';
                        }
                    }
                    
                    return div;
                };
                legend.addTo(map);
            }
        </script>
        '''
        
        return map_html
    except Exception as e:
        return f"<div>L·ªói khi t·∫°o b·∫£n ƒë·ªì: {str(e)}</div>"

def detect_anomalies(df):
    """Ph√°t hi·ªán d·ªØ li·ªáu b·∫•t th∆∞·ªùng"""
    try:
        # T√¨m c·ªôt t·ªëc ƒë·ªô
        speed_col = next((col for col in ['SOG', 'Speed', 'speed'] if col in df.columns), None)
        
        if not speed_col:
            return {"error": "Kh√¥ng t√¨m th·∫•y c·ªôt t·ªëc ƒë·ªô"}
        
        # L·ªçc d·ªØ li·ªáu h·ª£p l·ªá
        df_clean = df.dropna(subset=[speed_col])
        
        if len(df_clean) < 10:
            return {"error": "Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch"}
        
        # T√≠nh ng∆∞·ª°ng b·∫•t th∆∞·ªùng (ph∆∞∆°ng ph√°p IQR)
        Q1 = df_clean[speed_col].quantile(0.25)
        Q3 = df_clean[speed_col].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # Ph√°t hi·ªán b·∫•t th∆∞·ªùng
        anomalies = df_clean[(df_clean[speed_col] < lower_bound) | (df_clean[speed_col] > upper_bound)]
        
        # Th·ªëng k√™
        anomaly_stats = {
            "total_anomalies": len(anomalies),
            "anomaly_ratio": len(anomalies) / len(df_clean),
            "speed_threshold": {
                "lower": float(lower_bound),
                "upper": float(upper_bound)
            }
        }
        
        # T√¨m c·ªôt lo·∫°i t√†u
        vessel_col = next((col for col in ['VesselType', 'ShipType', 'vessel_type'] if col in df.columns), None)
        
        # Ph√¢n t√≠ch b·∫•t th∆∞·ªùng theo lo·∫°i t√†u
        if vessel_col:
            anomaly_by_type = anomalies[vessel_col].value_counts().to_dict()
            anomaly_stats["anomaly_by_type"] = anomaly_by_type
        
        return anomaly_stats
    except Exception as e:
        return {"error": str(e)}

def analyze_correlations(df):
    """Ph√¢n t√≠ch t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn"""
    try:
        # T√¨m c√°c c·ªôt s·ªë li·ªáu
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        
        # Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c√≥ √Ω nghƒ©a cho ph√¢n t√≠ch t∆∞∆°ng quan
        exclude_cols = ['MMSI', 'IMO', 'VesselID']
        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        if len(numeric_cols) < 2:
            return {"error": "Kh√¥ng ƒë·ªß d·ªØ li·ªáu s·ªë ƒë·ªÉ ph√¢n t√≠ch t∆∞∆°ng quan"}
        
        # T√≠nh ma tr·∫≠n t∆∞∆°ng quan
        corr_df = df[numeric_cols].corr().round(2)
        
        # Chuy·ªÉn ma tr·∫≠n t∆∞∆°ng quan th√†nh danh s√°ch c√°c c·∫∑p t∆∞∆°ng quan
        correlations = []
        for i in range(len(numeric_cols)):
            for j in range(i+1, len(numeric_cols)):
                col1 = numeric_cols[i]
                col2 = numeric_cols[j]
                corr_value = corr_df.loc[col1, col2]
                
                # Ch·ªâ l·∫•y c√°c t∆∞∆°ng quan m·∫°nh (tr·ªã tuy·ªát ƒë·ªëi > 0.3)
                if abs(corr_value) > 0.3:
                    correlations.append({
                        "variable1": col1,
                        "variable2": col2,
                        "correlation": float(corr_value),
                        "strength": "m·∫°nh" if abs(corr_value) > 0.7 else "trung b√¨nh",
                        "direction": "thu·∫≠n" if corr_value > 0 else "ngh·ªãch"
                    })
        
        # S·∫Øp x·∫øp theo ƒë·ªô m·∫°nh c·ªßa t∆∞∆°ng quan (gi·∫£m d·∫ßn)
        correlations.sort(key=lambda x: abs(x["correlation"]), reverse=True)
        
        # T·∫°o bi·ªÉu ƒë·ªì t∆∞∆°ng quan
        plt.figure(figsize=(10, 8))
        plt.matshow(corr_df, fignum=1, cmap='coolwarm', vmin=-1, vmax=1)
        plt.colorbar()
        plt.xticks(range(len(numeric_cols)), numeric_cols, rotation=90)
        plt.yticks(range(len(numeric_cols)), numeric_cols)
        
        # Th√™m gi√° tr·ªã t∆∞∆°ng quan v√†o bi·ªÉu ƒë·ªì
        for i in range(len(numeric_cols)):
            for j in range(len(numeric_cols)):
                plt.text(i, j, f"{corr_df.iloc[j, i]:.2f}", 
                         ha="center", va="center", 
                         color="white" if abs(corr_df.iloc[j, i]) > 0.5 else "black")
        
        plt.tight_layout()
        
        # Chuy·ªÉn bi·ªÉu ƒë·ªì th√†nh base64 ƒë·ªÉ hi·ªÉn th·ªã tr√™n web
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=100)
        buf.seek(0)
        correlation_chart = base64.b64encode(buf.read()).decode('utf-8')
        plt.close()
        
        return {
            "correlations": correlations,
            "total_correlations": len(correlations),
            "chart": correlation_chart
        }
    except Exception as e:
        return {"error": str(e)}

def analyze_temporal_patterns(df):
    """Ph√¢n t√≠ch m·∫´u theo th·ªùi gian"""
    try:
        # T√¨m c·ªôt th·ªùi gian
        time_col = next((col for col in ['BaseDateTime', 'DateTime', 'Timestamp', 'date_time'] if col in df.columns), None)
        
        if not time_col:
            return {"error": "Kh√¥ng t√¨m th·∫•y c·ªôt th·ªùi gian"}
        
        # Chuy·ªÉn ƒë·ªïi c·ªôt th·ªùi gian sang ƒë·ªãnh d·∫°ng datetime
        df['parsed_time'] = pd.to_datetime(df[time_col], errors='coerce')
        df_time = df.dropna(subset=['parsed_time'])
        
        if len(df_time) < 10:
            return {"error": "Kh√¥ng ƒë·ªß d·ªØ li·ªáu th·ªùi gian ƒë·ªÉ ph√¢n t√≠ch"}
        
        # Th√™m c√°c c·ªôt th·ªùi gian
        df_time['hour'] = df_time['parsed_time'].dt.hour
        df_time['day_of_week'] = df_time['parsed_time'].dt.dayofweek
        df_time['day_name'] = df_time['parsed_time'].dt.day_name()
        df_time['month'] = df_time['parsed_time'].dt.month
        
        # Ph√¢n t√≠ch theo gi·ªù trong ng√†y
        hourly_counts = df_time['hour'].value_counts().sort_index()
        
        # T√¨m gi·ªù cao ƒëi·ªÉm
        peak_hour = hourly_counts.idxmax()
        peak_count = hourly_counts.max()
        
        # Ph√¢n t√≠ch theo ng√†y trong tu·∫ßn
        daily_counts = df_time['day_of_week'].value_counts().sort_index()
        day_names = ['Th·ª© 2', 'Th·ª© 3', 'Th·ª© 4', 'Th·ª© 5', 'Th·ª© 6', 'Th·ª© 7', 'Ch·ªß nh·∫≠t']
        daily_data = {day_names[i]: int(daily_counts.get(i, 0)) for i in range(7)}
        
        # T√¨m ng√†y b·∫≠n r·ªôn nh·∫•t
        busiest_day_idx = daily_counts.idxmax() if not daily_counts.empty else 0
        busiest_day = day_names[busiest_day_idx]
        busiest_day_count = daily_counts.max() if not daily_counts.empty else 0
        
        # T·∫°o bi·ªÉu ƒë·ªì ph√¢n b·ªë theo gi·ªù
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.bar(hourly_counts.index, hourly_counts.values, color='skyblue')
        plt.title('Ph√¢n b·ªë theo gi·ªù trong ng√†y')
        plt.xlabel('Gi·ªù')
        plt.ylabel('S·ªë l∆∞·ª£ng')
        plt.xticks(range(0, 24, 2))
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        
        # T·∫°o bi·ªÉu ƒë·ªì ph√¢n b·ªë theo ng√†y trong tu·∫ßn
        plt.subplot(1, 2, 2)
        plt.bar(day_names, [daily_data.get(day, 0) for day in day_names], color='lightgreen')
        plt.title('Ph√¢n b·ªë theo ng√†y trong tu·∫ßn')
        plt.xlabel('Ng√†y')
        plt.ylabel('S·ªë l∆∞·ª£ng')
        plt.xticks(rotation=45)
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        
        # Chuy·ªÉn bi·ªÉu ƒë·ªì th√†nh base64 ƒë·ªÉ hi·ªÉn th·ªã tr√™n web
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=100)
        buf.seek(0)
        time_chart = base64.b64encode(buf.read()).decode('utf-8')
        plt.close()
        
        # T√≠m c√°c m·∫´u th·ªùi gian ƒë·∫∑c bi·ªát
        temporal_patterns = [
            {
                "type": "peak_hour",
                "description": f"Gi·ªù cao ƒëi·ªÉm l√† {peak_hour}h v·ªõi {peak_count} t√†u",
                "details": {
                    "hour": int(peak_hour),
                    "count": int(peak_count)
                }
            },
            {
                "type": "busiest_day",
                "description": f"Ng√†y b·∫≠n r·ªôn nh·∫•t l√† {busiest_day} v·ªõi {busiest_day_count} t√†u",
                "details": {
                    "day": busiest_day,
                    "count": int(busiest_day_count)
                }
            }
        ]
        
        # T√¨m c√°c kho·∫£ng th·ªùi gian c√≥ ho·∫°t ƒë·ªông b·∫•t th∆∞·ªùng
        # T√≠nh trung b√¨nh v√† ƒë·ªô l·ªách chu·∫©n c·ªßa s·ªë l∆∞·ª£ng theo gi·ªù
        mean_hourly = hourly_counts.mean()
        std_hourly = hourly_counts.std()
        
        # X√°c ƒë·ªãnh c√°c gi·ªù c√≥ ho·∫°t ƒë·ªông b·∫•t th∆∞·ªùng (> mean + 1.5*std)
        anomaly_hours = hourly_counts[hourly_counts > (mean_hourly + 1.5 * std_hourly)]
        
        for hour, count in anomaly_hours.items():
            temporal_patterns.append({
                "type": "anomaly_hour",
                "description": f"Ho·∫°t ƒë·ªông b·∫•t th∆∞·ªùng cao v√†o l√∫c {hour}h v·ªõi {count} t√†u",
                "details": {
                    "hour": int(hour),
                    "count": int(count),
                    "mean": float(mean_hourly),
                    "std": float(std_hourly)
                }
            })
        
        return {
            "temporal_patterns": temporal_patterns,
            "hourly_distribution": {str(h): int(hourly_counts.get(h, 0)) for h in range(24)},
            "daily_distribution": daily_data,
            "chart": time_chart
        }
    except Exception as e:
        return {"error": str(e)}

def detect_vessel_groups(df):
    """Ph√°t hi·ªán c√°c nh√≥m t√†u di chuy·ªÉn c√πng nhau"""
    try:
        # T√¨m c√°c c·ªôt c·∫ßn thi·∫øt
        lat_col = next((col for col in ['LAT', 'Latitude', 'lat', 'latitude'] if col in df.columns), None)
        lon_col = next((col for col in ['LON', 'Longitude', 'lon', 'longitude'] if col in df.columns), None)
        mmsi_col = next((col for col in ['MMSI', 'mmsi', 'VesselId'] if col in df.columns), None)
        time_col = next((col for col in ['BaseDateTime', 'DateTime', 'Timestamp', 'date_time'] if col in df.columns), None)
        
        if not all([lat_col, lon_col, mmsi_col]):
            return {"error": "Thi·∫øu c√°c c·ªôt d·ªØ li·ªáu c·∫ßn thi·∫øt"}
        
        # L·ªçc d·ªØ li·ªáu h·ª£p l·ªá
        df_clean = df.dropna(subset=[lat_col, lon_col, mmsi_col])
        df_clean = df_clean[(df_clean[lat_col] >= -90) & (df_clean[lat_col] <= 90)]
        df_clean = df_clean[(df_clean[lon_col] >= -180) & (df_clean[lon_col] <= 180)]
        
        if len(df_clean) < 20:
            return {"error": "Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√°t hi·ªán nh√≥m t√†u"}
        
        # N·∫øu c√≥ c·ªôt th·ªùi gian, s·∫Ω ph√¢n t√≠ch theo th·ªùi gian
        if time_col and time_col in df_clean.columns:
            df_clean['parsed_time'] = pd.to_datetime(df_clean[time_col], errors='coerce')
            df_clean = df_clean.dropna(subset=['parsed_time'])
            
            # L·∫•y m·∫´u d·ªØ li·ªáu g·∫ßn ƒë√¢y nh·∫•t (n·∫øu qu√° l·ªõn)
            if len(df_clean) > 1000:
                df_clean = df_clean.sort_values('parsed_time', ascending=False).head(1000)
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        coords = df_clean[[lat_col, lon_col]].values
        coords_scaled = StandardScaler().fit_transform(coords)
        
        # Ph√°t hi·ªán c·ª•m b·∫±ng DBSCAN
        db = DBSCAN(eps=0.1, min_samples=3).fit(coords_scaled)
        labels = db.labels_
        
        # Th√™m nh√£n c·ª•m v√†o d·ªØ li·ªáu
        df_clean['cluster'] = labels
        
        # S·ªë l∆∞·ª£ng c·ª•m (kh√¥ng t√≠nh nhi·ªÖu)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        
        if n_clusters == 0:
            return {"error": "Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c nh√≥m t√†u n√†o"}
        
        # Ph√¢n t√≠ch c√°c nh√≥m t√†u
        vessel_groups = []
        
        for i in range(n_clusters):
            cluster_data = df_clean[df_clean['cluster'] == i]
            
            # T√≠nh to√°n trung t√¢m c·ªßa c·ª•m
            center_lat = cluster_data[lat_col].mean()
            center_lon = cluster_data[lon_col].mean()
            
            # ƒê·∫øm s·ªë l∆∞·ª£ng t√†u duy nh·∫•t trong c·ª•m
            unique_vessels = cluster_data[mmsi_col].nunique()
            
            # X√°c ƒë·ªãnh lo·∫°i t√†u trong c·ª•m
            vessel_type_col = next((col for col in ['VesselType', 'ShipType', 'vessel_type'] if col in df_clean.columns), None)
            vessel_types = []
            if vessel_type_col:
                vessel_types = cluster_data[vessel_type_col].unique().tolist()
                vessel_types = [str(vt) for vt in vessel_types if pd.notna(vt)]
            
            # T√≠nh kho·∫£ng c√°ch trung b√¨nh gi·ªØa c√°c t√†u trong c·ª•m
            if len(cluster_data) > 1:
                from scipy.spatial.distance import pdist
                distances = pdist(cluster_data[[lat_col, lon_col]].values)
                avg_distance = float(np.mean(distances))
                # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch t·ª´ ƒë·ªô sang km (x·∫•p x·ªâ)
                avg_distance_km = avg_distance * 111  # 1 ƒë·ªô ~ 111km
            else:
                avg_distance_km = 0
            
            vessel_groups.append({
                "group_id": i,
                "center": [float(center_lat), float(center_lon)],
                "vessel_count": int(unique_vessels),
                "total_records": len(cluster_data),
                "vessel_types": vessel_types,
                "avg_distance_km": round(avg_distance_km, 2)
            })
        
        # S·∫Øp x·∫øp theo s·ªë l∆∞·ª£ng t√†u gi·∫£m d·∫ßn
        vessel_groups.sort(key=lambda x: x["vessel_count"], reverse=True)
        
        # T·∫°o b·∫£n ƒë·ªì hi·ªÉn th·ªã c√°c nh√≥m t√†u
        m = folium.Map(location=[df_clean[lat_col].mean(), df_clean[lon_col].mean()], zoom_start=8)
        
        # M√†u s·∫Øc cho c√°c c·ª•m
        colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'darkblue', 'darkgreen', 'cadetblue', 'darkpurple']
        
        # Th√™m c√°c ƒëi·ªÉm v√†o b·∫£n ƒë·ªì
        for i in range(n_clusters):
            cluster_data = df_clean[df_clean['cluster'] == i]
            color = colors[i % len(colors)]
            
            # T·∫°o feature group cho c·ª•m
            fg = folium.FeatureGroup(name=f"Nh√≥m {i+1} ({len(cluster_data)} ƒëi·ªÉm)")
            
            # Th√™m c√°c ƒëi·ªÉm v√†o feature group
            for _, row in cluster_data.iterrows():
                popup_text = f"<b>MMSI:</b> {row[mmsi_col]}<br>"
                
                if vessel_type_col and vessel_type_col in row:
                    popup_text += f"<b>Lo·∫°i t√†u:</b> {row[vessel_type_col]}<br>"
                
                popup_text += f"<b>V·ªã tr√≠:</b> {row[lat_col]:.4f}, {row[lon_col]:.4f}<br>"
                popup_text += f"<b>Nh√≥m:</b> {i+1}"
                
                folium.CircleMarker(
                    location=[row[lat_col], row[lon_col]],
                    radius=5,
                    popup=folium.Popup(popup_text, max_width=300),
                    color=color,
                    fill=True,
                    fill_color=color,
                    fill_opacity=0.7
                ).add_to(fg)
            
            # Th√™m ƒë∆∞·ªùng bao quanh c·ª•m
            if len(cluster_data) > 2:
                from scipy.spatial import ConvexHull
                points = cluster_data[[lon_col, lat_col]].values
                hull = ConvexHull(points)
                hull_points = [points[vertex] for vertex in hull.vertices]
                hull_points.append(hull_points[0])  # ƒê√≥ng ƒëa gi√°c
                
                # ƒê·ªïi th·ª© t·ª± t·ªça ƒë·ªô (lon, lat) -> (lat, lon) cho folium
                hull_points_folium = [[p[1], p[0]] for p in hull_points]
                
                folium.Polygon(
                    locations=hull_points_folium,
                    color=color,
                    weight=2,
                    fill=True,
                    fill_color=color,
                    fill_opacity=0.1,
                    popup=f"Nh√≥m {i+1}: {len(cluster_data)} t√†u"
                ).add_to(fg)
            
            fg.add_to(m)
        
        # Th√™m ƒëi·ªÅu khi·ªÉn l·ªõp
        folium.LayerControl().add_to(m)
        
        # Chuy·ªÉn b·∫£n ƒë·ªì th√†nh HTML
        groups_map_html = m._repr_html_()
        
        return {
            "vessel_groups": vessel_groups,
            "total_groups": n_clusters,
            "total_vessels_in_groups": sum(g["vessel_count"] for g in vessel_groups),
            "map_html": groups_map_html
        }
    except Exception as e:
        return {"error": str(e)}

def extract_hidden_patterns(df):
    """Khai ph√° c√°c m·∫´u ·∫©n trong d·ªØ li·ªáu"""
    try:
        # T√¨m c·ªôt t·ªça ƒë·ªô v√† th·ªùi gian
        lat_col = next((col for col in ['LAT', 'Latitude', 'lat', 'latitude'] if col in df.columns), None)
        lon_col = next((col for col in ['LON', 'Longitude', 'lon', 'longitude'] if col in df.columns), None)
        time_col = next((col for col in ['BaseDateTime', 'DateTime', 'Timestamp', 'date_time'] if col in df.columns), None)
        
        if not lat_col or not lon_col:
            return {"error": "Kh√¥ng t√¨m th·∫•y c·ªôt t·ªça ƒë·ªô"}
        
        # K·∫øt qu·∫£ ph√¢n t√≠ch
        insights = []
        
        # 1. Ph√¢n t√≠ch ph√¢n b·ªë ƒë·ªãa l√Ω
        df_clean = df.dropna(subset=[lat_col, lon_col])
        if len(df_clean) >= 10:
            # Chia th√†nh l∆∞·ªõi 4x4
            lat_bins = pd.cut(df_clean[lat_col], 4)
            lon_bins = pd.cut(df_clean[lon_col], 4)
            
            # ƒê·∫øm s·ªë l∆∞·ª£ng t√†u trong m·ªói √¥ l∆∞·ªõi
            grid_counts = df_clean.groupby([lat_bins, lon_bins]).size()
            
            # T√¨m √¥ l∆∞·ªõi c√≥ nhi·ªÅu t√†u nh·∫•t
            if not grid_counts.empty:
                max_grid = grid_counts.idxmax()
                max_count = grid_counts.max()
                
                insights.append({
                    "type": "geographic_hotspot",
                    "description": f"Ph√°t hi·ªán khu v·ª±c t·∫≠p trung cao v·ªõi {max_count} t√†u",
                    "details": {
                        "lat_range": str(max_grid[0]),
                        "lon_range": str(max_grid[1]),
                        "count": int(max_count)
                    }
                })
        
        # 2. Ph√¢n t√≠ch theo th·ªùi gian
        if time_col and time_col in df.columns:
            try:
                df['parsed_time'] = pd.to_datetime(df[time_col], errors='coerce')
                df_time = df.dropna(subset=['parsed_time'])
                
                if len(df_time) >= 10:
                    # Ph√¢n t√≠ch theo gi·ªù trong ng√†y
                    df_time['hour'] = df_time['parsed_time'].dt.hour
                    hourly_counts = df_time['hour'].value_counts().sort_index()
                    
                    # T√¨m gi·ªù cao ƒëi·ªÉm
                    peak_hour = hourly_counts.idxmax()
                    peak_count = hourly_counts.max()
                    
                    insights.append({
                        "type": "time_pattern",
                        "description": f"Gi·ªù cao ƒëi·ªÉm l√† {peak_hour}h v·ªõi {peak_count} t√†u",
                        "details": {
                            "peak_hour": int(peak_hour),
                            "count": int(peak_count)
                        }
                    })
            except:
                pass
        
        # 3. Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn
        speed_col = next((col for col in ['SOG', 'Speed', 'speed'] if col in df.columns), None)
        course_col = next((col for col in ['COG', 'Course', 'course'] if col in df.columns), None)
        
        if speed_col and course_col:
            df_nav = df.dropna(subset=[speed_col, course_col])
            
            if len(df_nav) >= 10:
                # T√≠nh h·ªá s·ªë t∆∞∆°ng quan
                correlation = df_nav[speed_col].corr(df_nav[course_col])
                
                if not pd.isna(correlation):
                    insights.append({
                        "type": "correlation",
                        "description": f"H·ªá s·ªë t∆∞∆°ng quan gi·ªØa t·ªëc ƒë·ªô v√† h∆∞·ªõng ƒëi l√† {correlation:.2f}",
                        "details": {
                            "correlation": float(correlation),
                            "variables": [speed_col, course_col]
                        }
                    })
        
        # 4. Ph√°t hi·ªán c√°c nh√≥m t√†u di chuy·ªÉn c√πng nhau
        vessel_col = next((col for col in ['VesselType', 'ShipType', 'vessel_type'] if col in df.columns), None)
        
        if vessel_col and lat_col and lon_col:
            # S·ª≠ d·ª•ng DBSCAN ƒë·ªÉ ph√°t hi·ªán c√°c nh√≥m t√†u g·∫ßn nhau
            df_pos = df.dropna(subset=[lat_col, lon_col, vessel_col])
            
            if len(df_pos) >= 20:
                # Chu·∫©n h√≥a d·ªØ li·ªáu
                coords = df_pos[[lat_col, lon_col]].values
                coords_scaled = StandardScaler().fit_transform(coords)
                
                # Ph√°t hi·ªán c·ª•m
                db = DBSCAN(eps=0.1, min_samples=3).fit(coords_scaled)
                labels = db.labels_
                
                # S·ªë l∆∞·ª£ng c·ª•m (kh√¥ng t√≠nh nhi·ªÖu)
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                
                if n_clusters > 0:
                    # Ph√¢n t√≠ch th√†nh ph·∫ßn c·ªßa c√°c c·ª•m
                    df_pos['cluster'] = labels
                    
                    # T√¨m c·ª•m c√≥ nhi·ªÅu lo·∫°i t√†u kh√°c nhau nh·∫•t
                    diverse_clusters = []
                    
                    for i in range(n_clusters):
                        cluster_data = df_pos[df_pos['cluster'] == i]
                        vessel_types = cluster_data[vessel_col].nunique()
                        
                        if vessel_types > 1:
                            diverse_clusters.append({
                                "cluster_id": i,
                                "vessel_types": int(vessel_types),
                                "total_vessels": len(cluster_data)
                            })
                    
                    if diverse_clusters:
                        # S·∫Øp x·∫øp theo s·ªë l∆∞·ª£ng lo·∫°i t√†u gi·∫£m d·∫ßn
                        diverse_clusters.sort(key=lambda x: x['vessel_types'], reverse=True)
                        top_cluster = diverse_clusters[0]
                        
                        insights.append({
                            "type": "vessel_group",
                            "description": f"Ph√°t hi·ªán nh√≥m {top_cluster['total_vessels']} t√†u thu·ªôc {top_cluster['vessel_types']} lo·∫°i kh√°c nhau di chuy·ªÉn g·∫ßn nhau",
                            "details": top_cluster
                        })
        
        return {
            "insights": insights,
            "total_insights": len(insights)
        }
    except Exception as e:
        return {"error": str(e)}